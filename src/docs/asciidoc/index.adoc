
= Java Deep Learning Inference Services
Doc Writer <christian.tzolov@mindmodel.io>
{docdate}
:revnumber: {project-version}

ifndef::imagesdir[:imagesdir: images]

ifndef::sourcedir[:sourcedir: ../../main/java]

ifdef::env-github[:imagesdir: /src/docs/asciidoc/images/images]

[role=right]
--
[cols=3, , frame=none, grid=none]
|===
|image:{imagesdir}/../GitHub-logo.png[link=https://github.com/tzolov/mind-model-services, width=20, role=left]
https://github.com/tzolov/mind-model-services[mind-model]
|image:{imagesdir}/../twitter_logo.png[link=https://twitter.com/christzolov, width=20, role=left]
https://twitter.com/christzolov[@christzolov]
|image:{imagesdir}/../linkedin-logo.png[link=https://www.linkedin.com/in/tzolov/, width=20, role=left]
https://www.linkedin.com/in/tzolov/[tzolov]
|===
--

:example-caption!:

User manual for the https://github.com/tzolov/mind-model-services[mind-model-services] collection of projects.

== Introduction

Java libraries for enabling and simplifying the inference of Deep Learning models.

image:{imagesdir}/../reference-arch.png[width=80%, role=right]
Provides Java wrappers for various state-of-the-art deep learning models. To be used in standalone Java
application, https://spring.io/projects/spring-boot[SpringBoot] microservices as well as https://cloud.spring.io/spring-cloud-stream/[Streaming],
https://spring.io/projects/spring-cloud-task#learn[Task/Batch] and https://cloud.spring.io/spring-cloud-function/[Serverless] programing models.

The implementations use the https://www.tensorflow.org/install/lang_java[TensorFlow Java API] as well as the
https://deeplearning4j.org/docs/latest/nd4j-overview[DL4J] API stack such as https://github.com/bytedeco/javacv[JavaCV],
https://deeplearning4j.org/docs/latest/nd4j-overview[ND4J] https://github.com/deeplearning4j/deeplearning4j/tree/master/nd4j/nd4j-tensorflow[nd4j-tensorflow].

The pre-trained TensorFlow models can be loaded from `classpath`, `file` or `http` resource locations.
Also the models can be loaded directly from their `model-zoo` pages! For this just use the
following URI convention:

----
http://<Model's tar.gz URL>#frozen_inference_graph.pb
----

Where the `frozen_inference_graph.pb` is the frozen model's file name within the archive specified with the URI.
Change the fragment name to match the file name within the model archive you use. The `frozen_inference_graph.pb`
seems to be used by default in many projects.

== Service Catalog

---
include::{docdir}/object-detection/README.adoc[]
---
include::{docdir}/semantic-segmentation/README.adoc[]
---
include::{docdir}/pose-estimation/README.adoc[]
---
include::{docdir}/face-detection-mtcnn/README.adoc[]
---
include::{docdir}/twitter-sentiment/README.adoc[]
---
include::{docdir}/image-intermediateResult/README.adoc[]
---



